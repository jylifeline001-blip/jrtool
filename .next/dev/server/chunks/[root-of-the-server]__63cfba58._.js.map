{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 52, "column": 0}, "map": {"version":3,"sources":["file:///D:/work/v0-domain-expiry-checker/app/api/health-check/route.ts"],"sourcesContent":["import { NextResponse } from \"next/server\"\r\nimport { JSDOM } from \"jsdom\"\r\n\r\nexport const maxDuration = 60 // Allow longer timeout for crawling\r\n\r\ninterface PageHealth {\r\n    url: string\r\n    status: \"healthy\" | \"warning\" | \"error\"\r\n    statusCode: number\r\n    loadTime: number\r\n    issues: string[]\r\n    issueType?: \"Speed\" | \"CSS\" | \"Content\" | \"Security\"\r\n}\r\n\r\n// Simple internal categorization for issues\r\nconst getIssueType = (issues: string[]): \"Speed\" | \"CSS\" | \"Content\" | \"Security\" | undefined => {\r\n    if (issues.some(i => i.toLowerCase().includes(\"security\") || i.toLowerCase().includes(\"hacked\") || i.toLowerCase().includes(\"malware\"))) return \"Security\"\r\n    if (issues.some(i => i.toLowerCase().includes(\"css\") || i.toLowerCase().includes(\"content\"))) return \"CSS\" // or Content\r\n    if (issues.some(i => i.toLowerCase().includes(\"slow\") || i.toLowerCase().includes(\"time\"))) return \"Speed\"\r\n    return undefined\r\n}\r\n\r\nasync function checkUrl(url: string, isMain: boolean = false): Promise<PageHealth> {\r\n    const issues: string[] = []\r\n    let status: \"healthy\" | \"warning\" | \"error\" = \"healthy\"\r\n    let statusCode = 0\r\n    const start = performance.now()\r\n\r\n    try {\r\n        const controller = new AbortController()\r\n        // Timeout 10s for individual page\r\n        const timeout = setTimeout(() => controller.abort(), 10000)\r\n\r\n        const res = await fetch(url, {\r\n            signal: controller.signal,\r\n            headers: {\r\n                \"User-Agent\": \"Mozilla/5.0 (compatible; JRTools-WebsiteHealthChecker/1.0)\",\r\n            },\r\n        })\r\n        clearTimeout(timeout)\r\n        const loadTime = Math.round(performance.now() - start)\r\n        statusCode = res.status\r\n\r\n        if (!res.ok) {\r\n            status = \"error\"\r\n            issues.push(`HTTP Status ${res.status}`)\r\n        } else {\r\n            // Check for redirects\r\n            if (res.redirected) {\r\n                const origin = new URL(url).origin\r\n                const finalOrigin = new URL(res.url).origin\r\n                if (origin !== finalOrigin) {\r\n                    status = \"warning\"\r\n                    issues.push(`Redirected to external domain: ${finalOrigin}`)\r\n                }\r\n            }\r\n        }\r\n\r\n        // Speed Check\r\n        if (loadTime > 3000) {\r\n            if (status !== \"error\") status = \"warning\"\r\n            issues.push(`Slow load time: ${(loadTime / 1000).toFixed(2)}s`)\r\n        }\r\n\r\n        // HTTPS Check\r\n        if (url.startsWith(\"http://\")) {\r\n            status = \"warning\"\r\n            issues.push(\"Not using HTTPS\")\r\n        }\r\n\r\n        let htmlContent = \"\"\r\n\r\n        // Deep checks: CSS, Content, Security\r\n        if (res.ok) {\r\n            const text = await res.text()\r\n            htmlContent = text\r\n            const lowerText = text.toLowerCase()\r\n\r\n            // 1. Content Check\r\n            if (text.length < 500) {\r\n                status = \"warning\"\r\n                issues.push(\"Page content is suspiciously short (< 500 chars)\")\r\n            }\r\n\r\n            // 2. Security Checks (Heuristic)\r\n            if (lowerText.includes(\"hacked by\") || lowerText.includes(\"0wn3d\")) {\r\n                status = \"error\"\r\n                issues.push(\"Potential defacement text found\")\r\n            }\r\n\r\n            // Check for specific malicious patterns (common in WP hacks)\r\n            if (text.match(/<script[^>]*>[\\s\\S]*?eval\\s*\\(/i)) {\r\n                status = \"warning\"\r\n                issues.push(\"Suspicious 'eval' in script tags\")\r\n            }\r\n\r\n            // 3. CSS availability\r\n            // While we are parsing, let's extract CSS links\r\n            const dom = new JSDOM(text)\r\n            const doc = dom.window.document\r\n            const links = Array.from(doc.querySelectorAll(\"link[rel='stylesheet']\")) as unknown as HTMLLinkElement[]\r\n\r\n            // Check first 3 CSS files to see if they are reachable (head request)\r\n            // Check max 3 to avoid hanging\r\n            let cssBroken = false\r\n            const cssChecks = links.slice(0, 3).map(async (link) => {\r\n                const cssHref = link.href\r\n                if (!cssHref) return\r\n                try {\r\n                    const fullCssUrl = new URL(cssHref, url).href\r\n                    const cssRes = await fetch(fullCssUrl, { method: \"HEAD\", signal: AbortSignal.timeout(3000) })\r\n                    if (!cssRes.ok) return { url: fullCssUrl, status: cssRes.status }\r\n                } catch (e) {\r\n                    return { url: cssHref, error: true }\r\n                }\r\n            })\r\n\r\n            const cssResults = await Promise.all(cssChecks)\r\n            cssResults.forEach(r => {\r\n                if (r) {\r\n                    status = \"warning\"\r\n                    issues.push(`Broken CSS: ${r.url} (${r.status || 'Error'})`)\r\n                    cssBroken = true\r\n                }\r\n            })\r\n        }\r\n\r\n        return {\r\n            url,\r\n            status,\r\n            statusCode,\r\n            loadTime,\r\n            issues,\r\n            issueType: getIssueType(issues),\r\n            html: htmlContent\r\n        } as any\r\n\r\n    } catch (error: any) {\r\n        return {\r\n            url,\r\n            status: \"error\",\r\n            statusCode: 0,\r\n            loadTime: Math.round(performance.now() - start),\r\n            issues: [`Unreachable: ${error.message}`]\r\n        }\r\n    }\r\n}\r\n\r\n// Function to fetch sitemap and return URLs\r\nasync function fetchSitemap(baseUrl: string): Promise<string[]> {\r\n    try {\r\n        const sitemapUrl = new URL(\"/sitemap.xml\", baseUrl).href\r\n        const res = await fetch(sitemapUrl, { signal: AbortSignal.timeout(5000) })\r\n        if (!res.ok) return []\r\n        const text = await res.text()\r\n\r\n        // Simple regex to extract <loc> URLs\r\n        const regex = /<loc>(.*?)<\\/loc>/g\r\n        const urls: string[] = []\r\n        let match\r\n        while ((match = regex.exec(text)) !== null) {\r\n            const foundUrl = match[1].trim()\r\n            // If it's a sitemap index (nested sitemap), we might want to fetch it too, but for speed we'll just ignore or add it if it looks like a page.\r\n            // A better check is to see if it ends in .xml\r\n            if (foundUrl.endsWith(\".xml\")) {\r\n                // It's likely a sitemap index. For V0 demo, we might skip deep recursion or just try to fetch this one too.\r\n                // Let's try to fetch one level deep.\r\n                try {\r\n                    const subRes = await fetch(foundUrl, { signal: AbortSignal.timeout(3000) })\r\n                    if (subRes.ok) {\r\n                        const subText = await subRes.text()\r\n                        let subMatch\r\n                        while ((subMatch = regex.exec(subText)) !== null) {\r\n                            urls.push(subMatch[1].trim())\r\n                        }\r\n                    }\r\n                } catch { }\r\n            } else {\r\n                urls.push(foundUrl)\r\n            }\r\n        }\r\n        return urls\r\n    } catch {\r\n        return []\r\n    }\r\n}\r\n\r\n// Crawler BFS\r\nasync function crawlWebsite(startUrl: string, maxPages: number = 30): Promise<string[]> {\r\n    const visited = new Set<string>()\r\n    const queue: string[] = [startUrl]\r\n    const foundUrls: string[] = []\r\n    const baseOrigin = new URL(startUrl).origin\r\n\r\n    visited.add(startUrl)\r\n\r\n    while (queue.length > 0 && foundUrls.length < maxPages) {\r\n        const currentUrl = queue.shift()!\r\n        // foundUrls should only contain unique verified links, but we add them as we pop from queue to avoid duplicates in scanning\r\n        // actually, we should track what we intend to scan.\r\n        foundUrls.push(currentUrl)\r\n\r\n        try {\r\n            const controller = new AbortController()\r\n            const timeout = setTimeout(() => controller.abort(), 8000) // Increase timeout for crawling\r\n            const res = await fetch(currentUrl, {\r\n                signal: controller.signal,\r\n                headers: {\r\n                    \"User-Agent\": \"Mozilla/5.0 (compatible; JRTools-WebsiteHealthChecker/1.0)\",\r\n                }\r\n            })\r\n            clearTimeout(timeout)\r\n\r\n            if (res.ok && res.headers.get(\"content-type\")?.includes(\"text/html\")) {\r\n                const text = await res.text()\r\n\r\n                // Use JSDOM for robust link parsing\r\n                const dom = new JSDOM(text)\r\n                const doc = dom.window.document\r\n                const anchors = Array.from(doc.querySelectorAll(\"a\")) as unknown as HTMLAnchorElement[]\r\n\r\n                for (const a of anchors) {\r\n                    const href = a.href || a.getAttribute(\"href\") // JSDOM might not resolve .href to absolute completely in this context without a base URL in constructor, but let's try\r\n\r\n                    if (!href || href.startsWith(\"#\") || href.startsWith(\"javascript:\") || href.startsWith(\"mailto:\")) continue\r\n\r\n                    try {\r\n                        const fullUrl = new URL(href, currentUrl).href\r\n\r\n                        // Strict internal link check\r\n                        if (new URL(fullUrl).origin === baseOrigin && !visited.has(fullUrl)) {\r\n                            // Filter files\r\n                            if (!fullUrl.match(/\\.(jpg|jpeg|png|gif|pdf|zip|css|js|xml|json|svg)$/i)) {\r\n                                visited.add(fullUrl)\r\n                                queue.push(fullUrl)\r\n                            }\r\n                        }\r\n                    } catch { }\r\n                }\r\n            }\r\n        } catch { }\r\n\r\n        // Yield to event loop\r\n        await new Promise(r => setTimeout(r, 50))\r\n    }\r\n\r\n    return foundUrls\r\n}\r\n\r\n\r\nexport async function POST(req: Request) {\r\n    try {\r\n        const { url } = await req.json()\r\n\r\n        if (!url) {\r\n            return NextResponse.json({ error: \"URL is required\" }, { status: 400 })\r\n        }\r\n\r\n        let targetUrl = url.trim()\r\n        if (!targetUrl.startsWith(\"http\")) {\r\n            targetUrl = `https://${targetUrl}`\r\n        }\r\n\r\n        // Attempt to discover pages\r\n        // 1. Sitemap\r\n        let pageUrls = await fetchSitemap(targetUrl)\r\n\r\n        // 2. Crawl if no sitemap or very few pages (heuristic: if sitemap has < 5 pages, it might be incomplete or just a sitemap index that failed)\r\n        // We combine results if sitemap is small\r\n        if (pageUrls.length < 5) {\r\n            const crawledUrls = await crawlWebsite(targetUrl, 25)\r\n            // Merge and dedupe\r\n            pageUrls = Array.from(new Set([...pageUrls, ...crawledUrls]))\r\n        } else {\r\n            // Limit large sitemaps\r\n            if (pageUrls.length > 50) pageUrls = pageUrls.slice(0, 50)\r\n        }\r\n\r\n        // Ensure main page is included\r\n        if (!pageUrls.includes(targetUrl) && !pageUrls.some(u => u.includes(targetUrl.replace(/^https?:\\/\\//, '')))) {\r\n            pageUrls.unshift(targetUrl)\r\n        }\r\n\r\n        // Remove duplicates again just in case\r\n        pageUrls = Array.from(new Set(pageUrls))\r\n\r\n        // Valid Limit to prevent timeout\r\n        if (pageUrls.length > 30) pageUrls = pageUrls.slice(0, 30)\r\n\r\n        // PERFORM CHECKS\r\n        const results: PageHealth[] = []\r\n        const batchSize = 5\r\n\r\n        for (let i = 0; i < pageUrls.length; i += batchSize) {\r\n            const batch = pageUrls.slice(i, i + batchSize)\r\n            const batchResults = await Promise.all(batch.map(u => checkUrl(u)))\r\n\r\n            // Remove the internal 'html' prop before sending\r\n            batchResults.forEach((r: any) => { delete r.html })\r\n\r\n            results.push(...batchResults)\r\n        }\r\n\r\n        // Final stats\r\n        const totalPages = results.length\r\n        const errors = results.filter(r => r.status === \"error\").length\r\n        const slow = results.filter(r => r.loadTime > 3000).length\r\n        const security = results.filter(r => r.issues.some(i => i.toLowerCase().includes(\"security\") || i.toLowerCase().includes(\"hacked\"))).length\r\n\r\n        // Main result categorization\r\n        const mainPageResult = results.find(r => r.url === targetUrl) || results[0]\r\n\r\n        return NextResponse.json({\r\n            main: mainPageResult,\r\n            pages: results, // Send ALL pages\r\n            stats: {\r\n                totalPages,\r\n                errors,\r\n                slow,\r\n                security\r\n            }\r\n        })\r\n\r\n    } catch (error: any) {\r\n        return NextResponse.json({ error: error.message }, { status: 500 })\r\n    }\r\n}\r\n"],"names":[],"mappings":";;;;;;AAAA;AACA;;;AAEO,MAAM,cAAc,GAAG,oCAAoC;;AAWlE,4CAA4C;AAC5C,MAAM,eAAe,CAAC;IAClB,IAAI,OAAO,IAAI,CAAC,CAAA,IAAK,EAAE,WAAW,GAAG,QAAQ,CAAC,eAAe,EAAE,WAAW,GAAG,QAAQ,CAAC,aAAa,EAAE,WAAW,GAAG,QAAQ,CAAC,aAAa,OAAO;IAChJ,IAAI,OAAO,IAAI,CAAC,CAAA,IAAK,EAAE,WAAW,GAAG,QAAQ,CAAC,UAAU,EAAE,WAAW,GAAG,QAAQ,CAAC,aAAa,OAAO,MAAM,aAAa;;IACxH,IAAI,OAAO,IAAI,CAAC,CAAA,IAAK,EAAE,WAAW,GAAG,QAAQ,CAAC,WAAW,EAAE,WAAW,GAAG,QAAQ,CAAC,UAAU,OAAO;IACnG,OAAO;AACX;AAEA,eAAe,SAAS,GAAW,EAAE,SAAkB,KAAK;IACxD,MAAM,SAAmB,EAAE;IAC3B,IAAI,SAA0C;IAC9C,IAAI,aAAa;IACjB,MAAM,QAAQ,YAAY,GAAG;IAE7B,IAAI;QACA,MAAM,aAAa,IAAI;QACvB,kCAAkC;QAClC,MAAM,UAAU,WAAW,IAAM,WAAW,KAAK,IAAI;QAErD,MAAM,MAAM,MAAM,MAAM,KAAK;YACzB,QAAQ,WAAW,MAAM;YACzB,SAAS;gBACL,cAAc;YAClB;QACJ;QACA,aAAa;QACb,MAAM,WAAW,KAAK,KAAK,CAAC,YAAY,GAAG,KAAK;QAChD,aAAa,IAAI,MAAM;QAEvB,IAAI,CAAC,IAAI,EAAE,EAAE;YACT,SAAS;YACT,OAAO,IAAI,CAAC,CAAC,YAAY,EAAE,IAAI,MAAM,EAAE;QAC3C,OAAO;YACH,sBAAsB;YACtB,IAAI,IAAI,UAAU,EAAE;gBAChB,MAAM,SAAS,IAAI,IAAI,KAAK,MAAM;gBAClC,MAAM,cAAc,IAAI,IAAI,IAAI,GAAG,EAAE,MAAM;gBAC3C,IAAI,WAAW,aAAa;oBACxB,SAAS;oBACT,OAAO,IAAI,CAAC,CAAC,+BAA+B,EAAE,aAAa;gBAC/D;YACJ;QACJ;QAEA,cAAc;QACd,IAAI,WAAW,MAAM;YACjB,IAAI,WAAW,SAAS,SAAS;YACjC,OAAO,IAAI,CAAC,CAAC,gBAAgB,EAAE,CAAC,WAAW,IAAI,EAAE,OAAO,CAAC,GAAG,CAAC,CAAC;QAClE;QAEA,cAAc;QACd,IAAI,IAAI,UAAU,CAAC,YAAY;YAC3B,SAAS;YACT,OAAO,IAAI,CAAC;QAChB;QAEA,IAAI,cAAc;QAElB,sCAAsC;QACtC,IAAI,IAAI,EAAE,EAAE;YACR,MAAM,OAAO,MAAM,IAAI,IAAI;YAC3B,cAAc;YACd,MAAM,YAAY,KAAK,WAAW;YAElC,mBAAmB;YACnB,IAAI,KAAK,MAAM,GAAG,KAAK;gBACnB,SAAS;gBACT,OAAO,IAAI,CAAC;YAChB;YAEA,iCAAiC;YACjC,IAAI,UAAU,QAAQ,CAAC,gBAAgB,UAAU,QAAQ,CAAC,UAAU;gBAChE,SAAS;gBACT,OAAO,IAAI,CAAC;YAChB;YAEA,6DAA6D;YAC7D,IAAI,KAAK,KAAK,CAAC,oCAAoC;gBAC/C,SAAS;gBACT,OAAO,IAAI,CAAC;YAChB;YAEA,sBAAsB;YACtB,gDAAgD;YAChD,MAAM,MAAM,IAAI,4GAAK,CAAC;YACtB,MAAM,MAAM,IAAI,MAAM,CAAC,QAAQ;YAC/B,MAAM,QAAQ,MAAM,IAAI,CAAC,IAAI,gBAAgB,CAAC;YAE9C,sEAAsE;YACtE,+BAA+B;YAC/B,IAAI,YAAY;YAChB,MAAM,YAAY,MAAM,KAAK,CAAC,GAAG,GAAG,GAAG,CAAC,OAAO;gBAC3C,MAAM,UAAU,KAAK,IAAI;gBACzB,IAAI,CAAC,SAAS;gBACd,IAAI;oBACA,MAAM,aAAa,IAAI,IAAI,SAAS,KAAK,IAAI;oBAC7C,MAAM,SAAS,MAAM,MAAM,YAAY;wBAAE,QAAQ;wBAAQ,QAAQ,YAAY,OAAO,CAAC;oBAAM;oBAC3F,IAAI,CAAC,OAAO,EAAE,EAAE,OAAO;wBAAE,KAAK;wBAAY,QAAQ,OAAO,MAAM;oBAAC;gBACpE,EAAE,OAAO,GAAG;oBACR,OAAO;wBAAE,KAAK;wBAAS,OAAO;oBAAK;gBACvC;YACJ;YAEA,MAAM,aAAa,MAAM,QAAQ,GAAG,CAAC;YACrC,WAAW,OAAO,CAAC,CAAA;gBACf,IAAI,GAAG;oBACH,SAAS;oBACT,OAAO,IAAI,CAAC,CAAC,YAAY,EAAE,EAAE,GAAG,CAAC,EAAE,EAAE,EAAE,MAAM,IAAI,QAAQ,CAAC,CAAC;oBAC3D,YAAY;gBAChB;YACJ;QACJ;QAEA,OAAO;YACH;YACA;YACA;YACA;YACA;YACA,WAAW,aAAa;YACxB,MAAM;QACV;IAEJ,EAAE,OAAO,OAAY;QACjB,OAAO;YACH;YACA,QAAQ;YACR,YAAY;YACZ,UAAU,KAAK,KAAK,CAAC,YAAY,GAAG,KAAK;YACzC,QAAQ;gBAAC,CAAC,aAAa,EAAE,MAAM,OAAO,EAAE;aAAC;QAC7C;IACJ;AACJ;AAEA,4CAA4C;AAC5C,eAAe,aAAa,OAAe;IACvC,IAAI;QACA,MAAM,aAAa,IAAI,IAAI,gBAAgB,SAAS,IAAI;QACxD,MAAM,MAAM,MAAM,MAAM,YAAY;YAAE,QAAQ,YAAY,OAAO,CAAC;QAAM;QACxE,IAAI,CAAC,IAAI,EAAE,EAAE,OAAO,EAAE;QACtB,MAAM,OAAO,MAAM,IAAI,IAAI;QAE3B,qCAAqC;QACrC,MAAM,QAAQ;QACd,MAAM,OAAiB,EAAE;QACzB,IAAI;QACJ,MAAO,CAAC,QAAQ,MAAM,IAAI,CAAC,KAAK,MAAM,KAAM;YACxC,MAAM,WAAW,KAAK,CAAC,EAAE,CAAC,IAAI;YAC9B,8IAA8I;YAC9I,8CAA8C;YAC9C,IAAI,SAAS,QAAQ,CAAC,SAAS;gBAC3B,4GAA4G;gBAC5G,qCAAqC;gBACrC,IAAI;oBACA,MAAM,SAAS,MAAM,MAAM,UAAU;wBAAE,QAAQ,YAAY,OAAO,CAAC;oBAAM;oBACzE,IAAI,OAAO,EAAE,EAAE;wBACX,MAAM,UAAU,MAAM,OAAO,IAAI;wBACjC,IAAI;wBACJ,MAAO,CAAC,WAAW,MAAM,IAAI,CAAC,QAAQ,MAAM,KAAM;4BAC9C,KAAK,IAAI,CAAC,QAAQ,CAAC,EAAE,CAAC,IAAI;wBAC9B;oBACJ;gBACJ,EAAE,OAAM,CAAE;YACd,OAAO;gBACH,KAAK,IAAI,CAAC;YACd;QACJ;QACA,OAAO;IACX,EAAE,OAAM;QACJ,OAAO,EAAE;IACb;AACJ;AAEA,cAAc;AACd,eAAe,aAAa,QAAgB,EAAE,WAAmB,EAAE;IAC/D,MAAM,UAAU,IAAI;IACpB,MAAM,QAAkB;QAAC;KAAS;IAClC,MAAM,YAAsB,EAAE;IAC9B,MAAM,aAAa,IAAI,IAAI,UAAU,MAAM;IAE3C,QAAQ,GAAG,CAAC;IAEZ,MAAO,MAAM,MAAM,GAAG,KAAK,UAAU,MAAM,GAAG,SAAU;QACpD,MAAM,aAAa,MAAM,KAAK;QAC9B,4HAA4H;QAC5H,oDAAoD;QACpD,UAAU,IAAI,CAAC;QAEf,IAAI;YACA,MAAM,aAAa,IAAI;YACvB,MAAM,UAAU,WAAW,IAAM,WAAW,KAAK,IAAI,MAAM,gCAAgC;;YAC3F,MAAM,MAAM,MAAM,MAAM,YAAY;gBAChC,QAAQ,WAAW,MAAM;gBACzB,SAAS;oBACL,cAAc;gBAClB;YACJ;YACA,aAAa;YAEb,IAAI,IAAI,EAAE,IAAI,IAAI,OAAO,CAAC,GAAG,CAAC,iBAAiB,SAAS,cAAc;gBAClE,MAAM,OAAO,MAAM,IAAI,IAAI;gBAE3B,oCAAoC;gBACpC,MAAM,MAAM,IAAI,4GAAK,CAAC;gBACtB,MAAM,MAAM,IAAI,MAAM,CAAC,QAAQ;gBAC/B,MAAM,UAAU,MAAM,IAAI,CAAC,IAAI,gBAAgB,CAAC;gBAEhD,KAAK,MAAM,KAAK,QAAS;oBACrB,MAAM,OAAO,EAAE,IAAI,IAAI,EAAE,YAAY,CAAC,QAAQ,wHAAwH;;oBAEtK,IAAI,CAAC,QAAQ,KAAK,UAAU,CAAC,QAAQ,KAAK,UAAU,CAAC,kBAAkB,KAAK,UAAU,CAAC,YAAY;oBAEnG,IAAI;wBACA,MAAM,UAAU,IAAI,IAAI,MAAM,YAAY,IAAI;wBAE9C,6BAA6B;wBAC7B,IAAI,IAAI,IAAI,SAAS,MAAM,KAAK,cAAc,CAAC,QAAQ,GAAG,CAAC,UAAU;4BACjE,eAAe;4BACf,IAAI,CAAC,QAAQ,KAAK,CAAC,uDAAuD;gCACtE,QAAQ,GAAG,CAAC;gCACZ,MAAM,IAAI,CAAC;4BACf;wBACJ;oBACJ,EAAE,OAAM,CAAE;gBACd;YACJ;QACJ,EAAE,OAAM,CAAE;QAEV,sBAAsB;QACtB,MAAM,IAAI,QAAQ,CAAA,IAAK,WAAW,GAAG;IACzC;IAEA,OAAO;AACX;AAGO,eAAe,KAAK,GAAY;IACnC,IAAI;QACA,MAAM,EAAE,GAAG,EAAE,GAAG,MAAM,IAAI,IAAI;QAE9B,IAAI,CAAC,KAAK;YACN,OAAO,gJAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAAkB,GAAG;gBAAE,QAAQ;YAAI;QACzE;QAEA,IAAI,YAAY,IAAI,IAAI;QACxB,IAAI,CAAC,UAAU,UAAU,CAAC,SAAS;YAC/B,YAAY,CAAC,QAAQ,EAAE,WAAW;QACtC;QAEA,4BAA4B;QAC5B,aAAa;QACb,IAAI,WAAW,MAAM,aAAa;QAElC,6IAA6I;QAC7I,yCAAyC;QACzC,IAAI,SAAS,MAAM,GAAG,GAAG;YACrB,MAAM,cAAc,MAAM,aAAa,WAAW;YAClD,mBAAmB;YACnB,WAAW,MAAM,IAAI,CAAC,IAAI,IAAI;mBAAI;mBAAa;aAAY;QAC/D,OAAO;YACH,uBAAuB;YACvB,IAAI,SAAS,MAAM,GAAG,IAAI,WAAW,SAAS,KAAK,CAAC,GAAG;QAC3D;QAEA,+BAA+B;QAC/B,IAAI,CAAC,SAAS,QAAQ,CAAC,cAAc,CAAC,SAAS,IAAI,CAAC,CAAA,IAAK,EAAE,QAAQ,CAAC,UAAU,OAAO,CAAC,gBAAgB,OAAO;YACzG,SAAS,OAAO,CAAC;QACrB;QAEA,uCAAuC;QACvC,WAAW,MAAM,IAAI,CAAC,IAAI,IAAI;QAE9B,iCAAiC;QACjC,IAAI,SAAS,MAAM,GAAG,IAAI,WAAW,SAAS,KAAK,CAAC,GAAG;QAEvD,iBAAiB;QACjB,MAAM,UAAwB,EAAE;QAChC,MAAM,YAAY;QAElB,IAAK,IAAI,IAAI,GAAG,IAAI,SAAS,MAAM,EAAE,KAAK,UAAW;YACjD,MAAM,QAAQ,SAAS,KAAK,CAAC,GAAG,IAAI;YACpC,MAAM,eAAe,MAAM,QAAQ,GAAG,CAAC,MAAM,GAAG,CAAC,CAAA,IAAK,SAAS;YAE/D,iDAAiD;YACjD,aAAa,OAAO,CAAC,CAAC;gBAAa,OAAO,EAAE,IAAI;YAAC;YAEjD,QAAQ,IAAI,IAAI;QACpB;QAEA,cAAc;QACd,MAAM,aAAa,QAAQ,MAAM;QACjC,MAAM,SAAS,QAAQ,MAAM,CAAC,CAAA,IAAK,EAAE,MAAM,KAAK,SAAS,MAAM;QAC/D,MAAM,OAAO,QAAQ,MAAM,CAAC,CAAA,IAAK,EAAE,QAAQ,GAAG,MAAM,MAAM;QAC1D,MAAM,WAAW,QAAQ,MAAM,CAAC,CAAA,IAAK,EAAE,MAAM,CAAC,IAAI,CAAC,CAAA,IAAK,EAAE,WAAW,GAAG,QAAQ,CAAC,eAAe,EAAE,WAAW,GAAG,QAAQ,CAAC,YAAY,MAAM;QAE3I,6BAA6B;QAC7B,MAAM,iBAAiB,QAAQ,IAAI,CAAC,CAAA,IAAK,EAAE,GAAG,KAAK,cAAc,OAAO,CAAC,EAAE;QAE3E,OAAO,gJAAY,CAAC,IAAI,CAAC;YACrB,MAAM;YACN,OAAO;YACP,OAAO;gBACH;gBACA;gBACA;gBACA;YACJ;QACJ;IAEJ,EAAE,OAAO,OAAY;QACjB,OAAO,gJAAY,CAAC,IAAI,CAAC;YAAE,OAAO,MAAM,OAAO;QAAC,GAAG;YAAE,QAAQ;QAAI;IACrE;AACJ"}}]
}